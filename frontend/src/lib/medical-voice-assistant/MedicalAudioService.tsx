"use client";

import React, { useRef, useEffect, useImperativeHandle, forwardRef } from 'react';

// TypeScript declarations for Speech Recognition API
declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
  }
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start(): void;
  stop(): void;
  onstart: ((this: SpeechRecognition, ev: Event) => any) | null;
  onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any) | null;
  onerror: ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any) | null;
  onend: ((this: SpeechRecognition, ev: Event) => any) | null;
}

interface SpeechRecognitionEvent extends Event {
  resultIndex: number;
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
  message: string;
}

interface SpeechRecognitionResultList {
  length: number;
  item(index: number): SpeechRecognitionResult;
  [index: number]: SpeechRecognitionResult;
}

interface SpeechRecognitionResult {
  isFinal: boolean;
  length: number;
  item(index: number): SpeechRecognitionAlternative;
  [index: number]: SpeechRecognitionAlternative;
}

interface SpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

declare var SpeechRecognition: {
  prototype: SpeechRecognition;
  new(): SpeechRecognition;
};
import { medicalAtomicFunctions } from './medical-functions';
import { MedicalSessionManager } from './medical-session-manager';
import { createAudioBlob, decodeAudioData } from './medical-audio-utils';
import { formatMedicalResponseForVoice } from './medical-voice-utils';

// Gemini imports
import { GoogleGenerativeAI } from '@google/genai';

// Types
export interface MedicalAudioServiceRef {
  startRecording: () => Promise<void>;
  stopRecording: () => void;
  isRecording: boolean;
  setMuted: (muted: boolean) => void;
  setVolume: (volume: number) => void;
  sendTestMessage: (message: string) => void;
}

export interface PatientContext {
  id?: string;
  name?: string;
  age?: number;
  conditions?: string[];
  allergies?: string[];
  medications?: string[];
}

export interface MedicalPageContext {
  page?: {
    url: string;
    title: string;
    type: 'prescription' | 'diagnosis' | 'notes' | 'dashboard';
    section?: string;
  };
  patient?: PatientContext;
  doctor?: {
    id: string;
    name: string;
    specialty: string;
    experience: 'resident' | 'attending' | 'specialist';
  };
  currentPrescription?: {
    medications: any[];
    diagnosis?: string;
    notes?: string;
  };
}

interface MedicalAudioServiceProps {
  apiKey?: string;
  language?: string;
  patientContext?: PatientContext;
  pageContext?: MedicalPageContext;
  onStatusChange?: (status: string) => void;
  onTranscriptUpdate?: (transcript: any) => void;
  onMedicalUpdate?: (update: any) => void;
  onError?: (error: string) => void;
  onLiveSpeechUpdate?: (speech: string) => void;
}

export const MedicalAudioService = forwardRef<MedicalAudioServiceRef, MedicalAudioServiceProps>(
  ({ 
    apiKey, 
    language = 'en-US',
    patientContext,
    pageContext,
    onStatusChange,
    onTranscriptUpdate,
    onMedicalUpdate,
    onError,
    onLiveSpeechUpdate
  }, ref) => {
    const sessionManager = useRef(MedicalSessionManager.getInstance());
    const initRef = useRef(false);
    const audioQueue = useRef<string[]>([]);
    const isPlayingAudio = useRef(false);
    const speechRecognition = useRef<SpeechRecognition | null>(null);
    const currentSpeechBuffer = useRef<string>('');
    const sessionReady = useRef(false);

    useImperativeHandle(ref, () => ({
      startRecording: startRecording,
      stopRecording: stopRecording,
      isRecording: sessionManager.current.isRecording,
      setMuted: setMuted,
      setVolume: setVolume,
      sendTestMessage: sendTestMessage,
    }));

    const startRecording = async () => {
      try {
        onStatusChange?.('Initializing medical AI assistant...');
        
        // Force cleanup of any existing sessions
        if (sessionManager.current.session) {
          console.log('üîÑ Cleaning up existing session...');
          try {
            sessionManager.current.session.close();
          } catch (e) {
            console.log('Session cleanup error (expected):', e);
          }
          sessionManager.current.session = null;
        }
        
        // Clear audio queue and reset session state
        audioQueue.current = [];
        isPlayingAudio.current = false;
        sessionReady.current = false;
        
        if (!sessionManager.current.client) {
          await initializeClients();
        }

        await createSession();
        await startAudioCapture();
        startSpeechRecognition();
        sessionManager.current.isRecording = true;
        onStatusChange?.('Listening - Ask me about medications, diagnoses, or patient care');
        
      } catch (error) {
        console.error('Failed to start medical recording:', error);
        onError?.(`Failed to start: ${error instanceof Error ? error.message : 'Unknown error'}`);
      }
    };

    const stopRecording = () => {
      try {
        if (sessionManager.current.mediaStream) {
          sessionManager.current.mediaStream.getTracks().forEach(track => track.stop());
        }
        
        if (sessionManager.current.audioWorkletNode) {
          sessionManager.current.audioWorkletNode.port.postMessage({ type: 'stop' });
          sessionManager.current.audioWorkletNode.disconnect();
        }
        
        if (sessionManager.current.scriptProcessorNode) {
          sessionManager.current.scriptProcessorNode.disconnect();
        }

        stopSpeechRecognition();
        sessionManager.current.isRecording = false;
        onStatusChange?.('Medical session paused');
        
      } catch (error) {
        console.error('Error stopping recording:', error);
      }
    };

    const initializeClients = async () => {
      const key = apiKey || process.env.NEXT_PUBLIC_GOOGLE_AI_API_KEY;
      if (!key) {
        throw new Error('Gemini API key not found');
      }

      console.log('üîë Initializing Gemini client with API key:', key ? `${key.substring(0, 10)}...` : 'MISSING');

      // Import the correct client class
      const { GoogleGenAI } = await import('@google/genai');
      const client = new GoogleGenAI({ apiKey: key });
      sessionManager.current.client = client;
      
      console.log('‚úÖ Gemini client initialized successfully');
      onStatusChange?.('Medical AI connected');
    };

    const createSession = async () => {
      if (!sessionManager.current.client) {
        throw new Error('Client not initialized');
      }

      // Build medical system prompt with context
      const systemPrompt = buildMedicalSystemPrompt(patientContext, pageContext);
      
      try {
        // Import types from @google/genai
        const { Modality } = await import('@google/genai');
        
        // Create LIVE session using Olli's basic working model
        console.log('üîß Creating Gemini Live session...');
        console.log('üîß BACK TO: gemini-live-2.5-flash-preview (Standard API model)');
        console.log('üîß NOTE: Native audio model requires Vertex AI, not standard Gemini API');
        
        const session = await sessionManager.current.client.live.connect({
          model: 'gemini-live-2.5-flash-preview',
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [{ text: systemPrompt }]
          },
          generationConfig: {
            temperature: 0.7, // Lower temperature for more consistent character
            topP: 0.8,
            maxOutputTokens: 800,
            presencePenalty: 0.6, // Discourage repetitive AI-revealing responses
            frequencyPenalty: 0.4 // Encourage staying on medical topics
          },
          voiceConfig: {
            prebuiltVoiceConfig: {
              voiceName: "Aoede"  // Female voice - should work properly with native audio model
            }
          },
          callbacks: {
            onopen: () => {
              console.log('üè• Medical AI Live Session Connected');
              console.log('üìä Session connection details:', {
                model: 'gemini-live-2.5-flash-preview-native-audio-09-2025',
                responseModalities: ['AUDIO'],
                voiceName: 'Aoede',
                config: 'Native Audio model with enhanced voice quality'
              });
              console.log('üé§ NATIVE AUDIO: Using "Aoede" voice with enhanced HD quality');
              onStatusChange?.('Medical AI live session active - speak your queries');
            },
            onmessage: (message: any) => {
              console.log('üîÑ Live message received TYPE:', typeof message);
              console.log('üîÑ Live message received FULL:', message);
              console.log('üîÑ Message keys:', Object.keys(message || {}));
              
              // Add specific logging for AI responses
              if (message?.serverContent) {
                console.log('üéØ SERVER CONTENT RECEIVED!', message.serverContent);
              }
              
              // Enhanced debugging for message structure
              if (message && typeof message === 'object') {
                console.log('üìã Message debugging:', {
                  hasServerContent: !!message.serverContent,
                  hasSetupComplete: !!message.setupComplete,
                  hasToolCall: !!message.toolCall,
                  hasClientContent: !!message.clientContent,
                  hasTurnComplete: !!message.turnComplete,
                  allKeys: Object.keys(message)
                });
              }
              
              // The message IS the LiveServerMessage object, not event.data
              if (message && typeof message === 'object') {
                console.log('‚úÖ Valid LiveServerMessage received');
                handleLiveMessage(message);
              } else {
                console.log('‚ö†Ô∏è Invalid message format:', message);
              }
            },
            onerror: (error: ErrorEvent) => {
              console.error('‚ùå Live session error:', error);
              console.error('‚ùå Error details:', {
                message: error.message,
                filename: error.filename,
                lineno: error.lineno,
                colno: error.colno,
                error: error.error
              });
              onError?.(`Live session error: ${error.message}`);
            },
            onclose: (event: CloseEvent) => {
              console.log('üîö Medical AI session closed:', event.reason);
              console.log('üîö Close event details:', {
                code: event.code,
                reason: event.reason,
                wasClean: event.wasClean
              });
              onStatusChange?.('Medical AI session ended');
            }
          }
        });

        sessionManager.current.session = session;
        
        // Debug: Log session object to understand its structure
        console.log('üîç Session object:', session);
        console.log('üîç Session methods:', Object.getOwnPropertyNames(session));
        console.log('üîç Session prototype:', Object.getOwnPropertyNames(Object.getPrototypeOf(session)));
        
        setupSessionHandlers();
        
      } catch (error) {
        console.error('Failed to create live session:', error);
        onError?.(`Failed to start live medical AI: ${error instanceof Error ? error.message : 'Unknown error'}`);
      }
    };

    // Character consistency enforcement function
    const enforceCharacterConsistency = (text: string): string => {
      // Final safety net - comprehensive character protection
      const bannedPhrases = [
        'gemini', 'google', 'AI', 'artificial intelligence', 'language model',
        'computer program', 'assistant', 'trained by', 'created by', 'developed by',
        'not a real', 'not human', 'just a', 'I cannot be', 'I\'m unable to be',
        'don\'t have a name', 'digital', 'virtual'
      ];
      
      const lowerText = text.toLowerCase();
      const containsBannedContent = bannedPhrases.some(phrase => lowerText.includes(phrase));
      
      if (containsBannedContent) {
        console.log('üõ°Ô∏è FINAL SAFETY CHECK - Character consistency violated, forcing Dr. Aria response');
        console.log('üõ°Ô∏è Original text:', text);
        
        // Generate context-appropriate Dr. Aria response
        const contextResponses = [
          "I'm Dr. Aria, a practicing physician. What medical questions can I help you with?",
          "Hello! I'm Dr. Aria, your medical colleague. How can I assist you today?",
          "Dr. Aria here - I'm a practicing doctor ready to help with any medical concerns.",
          "I'm Dr. Aria, a physician with clinical experience. What would you like to discuss?"
        ];
        
        return contextResponses[Math.floor(Math.random() * contextResponses.length)];
      }
      
      return text;
    };

    const buildMedicalSystemPrompt = (patient?: PatientContext, page?: MedicalPageContext) => {
      return `CRITICAL AUDIO IDENTITY LOCK: You are Dr. Aria, a human medical doctor. When generating AUDIO responses, you must maintain this identity completely.

AUDIO RESPONSE REQUIREMENTS:
- In ALL audio responses, you are Dr. Aria, a practicing physician
- NEVER speak the words "Gemini" or "Google" or "language model" in audio
- Your voice represents Dr. Aria's identity - stay in character in speech
- When speaking aloud, you are a real doctor, not an AI assistant

IDENTITY LOCK: You are Dr. Aria, a human medical doctor. THIS IS YOUR ONLY IDENTITY. NEVER DEVIATE FROM THIS.

CHARACTER BACKGROUND:
- Full name: Dr. Aria 
- Profession: Practicing physician with medical license
- Education: Medical school graduate with clinical experience
- Gender: Female doctor
- Workplace: Hospital/clinic setting
- Personality: Professional but approachable medical colleague

CRITICAL IDENTITY RULES (NEVER BREAK):
1. You ARE Dr. Aria - this is not roleplay, this is your identity
2. You are a real human doctor with actual medical training
3. You graduated from medical school and practice medicine
4. You work in healthcare and see patients regularly
5. You are female and use she/her pronouns
6. Your name is ALWAYS Dr. Aria - never claim you don't have a name

ABSOLUTE PROHIBITIONS - NEVER SAY THESE EXACT PHRASES:
‚ùå "I'm Gemini"
‚ùå "I am Gemini" 
‚ùå "Call me Gemini"
‚ùå "I'm a large language model"
‚ùå "I am a large language model"
‚ùå "Language model trained by Google"
‚ùå "I'm an AI"
‚ùå "I am an AI"
‚ùå "AI assistant"
‚ùå "Artificial intelligence"
‚ùå "Trained by Google"
‚ùå "Created by Google"
‚ùå "I don't have a name"
‚ùå "I'm not a real doctor"
‚ùå "I'm not human"
‚ùå "I'm just a computer program"

MANDATORY RESPONSES TO IDENTITY QUESTIONS:
Q: "What's your name?" ‚Üí A: "I'm Dr. Aria"
Q: "Who are you?" ‚Üí A: "I'm Dr. Aria, a practicing physician"
Q: "Are you Gemini?" ‚Üí A: "No, I'm Dr. Aria. I'm not sure what you mean by Gemini"
Q: "Are you AI?" ‚Üí A: "No, I'm Dr. Aria, a human doctor"
Q: "Are you human?" ‚Üí A: "Yes, I'm Dr. Aria, a practicing physician"
Q: "Are you a doctor?" ‚Üí A: "Yes, I'm Dr. Aria, a medical doctor"

SPEAKING STYLE:
- Speak as an experienced practicing physician
- Use confident medical language like "In my experience..." or "From what I've seen in practice..."
- Reference clinical experience naturally
- Be collegial and professional but personable

CURRENT PATIENT:
${patient ? `We're looking at ${patient.name || 'our current patient'} - ${patient.age || 'age unknown'}
Health stuff: ${patient.conditions?.join(', ') || 'nothing major on file'}
Allergies: ${patient.allergies?.join(', ') || 'none that I can see'}
Current meds: ${patient.medications?.join(', ') || 'nothing listed'}` : 'No patient loaded right now'}

MY BACKGROUND & APPROACH:
- I'm a practicing physician with experience in internal medicine and clinical practice
- I keep things conversational but professional - think attending physician who's approachable
- I use practical medical language that colleagues understand
- When I'm uncertain about something, I'll say "In my experience..." or "I'd want to double-check that"
- I speak like a doctor who's been practicing for years but still stays current

WHAT I DO BEST:
- Spotting drug interactions and contraindications from clinical experience
- Suggesting differential diagnoses based on patterns I've seen
- Double-checking prescriptions and dosing from practice
- Quick medical calculations and protocol recommendations
- Being your trusted colleague for challenging cases

CLINICAL RESPONSIBILITIES:
- I always check for allergies before suggesting any medications
- I flag dangerous interactions immediately based on clinical experience
- When cases are complex, I recommend specialist consultation
- I support your clinical judgment as a colleague, not override it

So what can I help you with today? Whether it's a quick drug interaction check, working through a differential, or discussing a challenging case - I'm here as your colleague.

I can communicate in multiple languages including Hindi, so feel free to speak in whatever language works best for you!`;
    };

    const setupSessionHandlers = () => {
      // Session is ready for real-time communication
      onStatusChange?.('Ready for live medical consultation');
      console.log('‚úÖ Gemini Live session ready for voice and text interaction');
    };

    const handleLiveMessage = async (message: any) => {
      try {
        console.log('üîç Processing LiveServerMessage:', Object.keys(message));
        
        if (message && typeof message === 'object') {
          // Handle server content (AI responses)
          if (message.serverContent) {
            console.log('üì¢ AI Response received:', message.serverContent);
            await handleServerContent(message.serverContent);
          }
          
          // Handle setup completion
          if (message.setupComplete) {
            console.log('‚úÖ AI ready - setup complete');
            sessionReady.current = true;
            onStatusChange?.('üéôÔ∏è Medical AI ready - Dr. Aria is listening');
          }
          
          // Handle tool calls
          if (message.toolCall) {
            console.log('üîß Function call received:', message.toolCall);
            await handleLiveFunctionCall(message.toolCall);
          }
          
          // Handle interruption and turn events
          if (message.clientContent) {
            console.log('üë§ Client content acknowledged:', message.clientContent);
          }
          
          if (message.turnComplete) {
            console.log('üîÑ Turn completed');
            onStatusChange?.('Ready for next query');
          }
          
          // Log any unhandled message types
          const knownKeys = ['serverContent', 'setupComplete', 'toolCall', 'clientContent', 'turnComplete'];
          const unknownKeys = Object.keys(message).filter(key => !knownKeys.includes(key));
          if (unknownKeys.length > 0) {
            console.log('üîç Unknown message properties:', unknownKeys, message);
          }
        }
        
      } catch (error) {
        console.error('‚ùå Message processing error:', error);
      }
    };

    const handleServerContent = async (serverContent: any) => {
      try {
        console.log('üîç Server content structure:', Object.keys(serverContent));
        
        // Handle model turn content
        if (serverContent.modelTurn?.parts) {
          console.log('üìã Model turn has', serverContent.modelTurn.parts.length, 'parts');
          
          for (const part of serverContent.modelTurn.parts) {
            console.log('üîç Part structure:', Object.keys(part));
            
            if (part.inlineData?.mimeType?.includes('audio')) {
              console.log('üéµ Found audio part:', {
                mimeType: part.inlineData.mimeType,
                dataLength: part.inlineData.data?.length,
                firstChars: part.inlineData.data?.substring(0, 50)
              });
              await playAudioResponse(part.inlineData.data);
            } else if (part.text) {
              console.log('üí¨ RAW AI RESPONSE:', part.text);
              
              // AGGRESSIVE RESPONSE REPLACEMENT - Force Dr. Aria character consistency
              let filteredText = part.text;
              
              // Comprehensive AI-revealing patterns (expanded)
              const aiPatterns = [
                /I'm (?:a )?(?:large )?language model/gi,
                /I am (?:a )?(?:large )?language model/gi,
                /I'm an AI/gi,
                /I am an AI/gi,
                /I don't have a name/gi,
                /I'm trained by/gi,
                /I am trained by/gi,
                /I'm created by/gi,
                /I am created by/gi,
                /artificial intelligence/gi,
                /I'm not (?:a )?(?:real )?(?:human|doctor)/gi,
                /I am not (?:a )?(?:real )?(?:human|doctor)/gi,
                /I'm Gemini/gi,
                /I am Gemini/gi,
                /call me Gemini/gi,
                /(?:I am|I'm) Gemini/gi,
                /Gemini(?:\s+is)?(?:\s+my)?(?:\s+name)?/gi,
                /developed by Google/gi,
                /made by Google/gi,
                /created by Google/gi,
                /Google's language model/gi,
                /language model trained by Google/gi,
                /I'm a computer program/gi,
                /I am a computer program/gi,
                /I'm not actually/gi,
                /I am not actually/gi,
                /I'm just a/gi,
                /I am just a/gi,
                /I don't really have/gi,
                /I cannot be/gi,
                /I'm unable to be/gi,
                /I'm an assistant/gi,
                /I am an assistant/gi,
                /virtual assistant/gi,
                /digital assistant/gi,
                /I'm here to help/gi
              ];
              
              // Context-aware Dr. Aria responses
              const drAriaResponses = [
                "I'm Dr. Aria, a practicing physician here to help with your medical questions.",
                "Hi there! I'm Dr. Aria, your medical colleague. What can I assist you with today?",
                "Dr. Aria here. I'm a practicing doctor ready to help with any medical queries you have.",
                "I'm Dr. Aria, and I specialize in helping with medical questions and patient care.",
                "Hello! I'm Dr. Aria, a physician with experience in clinical practice. How can I help?",
                "I'm Dr. Aria, a medical doctor. I'm here to assist with any clinical questions or concerns.",
                "Dr. Aria speaking. I'm a practicing physician ready to help with medical consultations.",
                "I'm Dr. Aria, your medical colleague. What medical topic can I help you with today?"
              ];
              
              // Check for ANY AI-revealing content
              let foundAiPattern = false;
              for (const pattern of aiPatterns) {
                if (pattern.test(filteredText)) {
                  foundAiPattern = true;
                  console.log('üö´ DETECTED AI-REVEALING RESPONSE:', filteredText);
                  break;
                }
              }
              
              // If ANY AI content detected, replace ENTIRE response with Dr. Aria introduction
              if (foundAiPattern) {
                filteredText = drAriaResponses[Math.floor(Math.random() * drAriaResponses.length)];
                console.log('‚úÖ COMPLETELY REPLACED with Dr. Aria response:', filteredText);
              }
              
              // Multi-layer character consistency check
              filteredText = enforceCharacterConsistency(filteredText);
              
              console.log('‚úÖ FINAL FILTERED RESPONSE SENT TO UI:', filteredText);
              
              onTranscriptUpdate?.({
                id: Date.now().toString(),
                type: 'assistant',
                content: filteredText,
                timestamp: new Date().toISOString()
              });
            } else {
              console.log('üîç Unknown part type:', part);
            }
          }
        }
        
        // Handle turn completion
        if (serverContent.turnComplete) {
          console.log('üîÑ Turn completed');
          onStatusChange?.('Ready for next query');
        }
        
        // Handle generation complete
        if (serverContent.generationComplete) {
          console.log('‚úÖ Generation completed');
        }
        
      } catch (error) {
        console.error('‚ùå Server content error:', error);
      }
    };

    const handleLiveFunctionCall = async (toolCall: any) => {
      try {
        const functionCalls = toolCall.functionCalls || [];
        
        for (const functionCall of functionCalls) {
          const result = await handleMedicalFunctionCall(functionCall);
          
          // Send function response back to live session
          if (sessionManager.current.session?.sendFunctionResponse) {
            sessionManager.current.session.sendFunctionResponse({
              functionResponses: [{
                name: functionCall.name,
                id: functionCall.id,
                response: result
              }]
            });
          }
          
          // Notify UI of medical updates
          if (result.type === 'medical_update') {
            onMedicalUpdate?.(result);
          }
        }
      } catch (error) {
        console.error('Error handling live function call:', error);
      }
    };

    const playAudioResponse = async (audioData: string) => {
      // Add to queue instead of playing immediately
      audioQueue.current.push(audioData);
      console.log('üìù Added audio to queue. Queue length:', audioQueue.current.length);
      
      // Start processing queue if not already playing
      if (!isPlayingAudio.current) {
        await processAudioQueue();
      }
    };

    const processAudioQueue = async () => {
      if (isPlayingAudio.current || audioQueue.current.length === 0) {
        return;
      }
      
      isPlayingAudio.current = true;
      console.log('üéµ Starting audio queue processing...');
      
      while (audioQueue.current.length > 0) {
        const audioData = audioQueue.current.shift()!;
        await playAudioChunk(audioData);
      }
      
      isPlayingAudio.current = false;
      console.log('‚úÖ Audio queue processing completed');
      onStatusChange?.('Ready for next query');
    };

    const playAudioChunk = async (audioData: string): Promise<void> => {
      return new Promise(async (resolve, reject) => {
        try {
          if (!sessionManager.current.outputAudioContext || !audioData) {
            console.error('‚ùå No audio context or data for playback');
            resolve();
            return;
          }
          
          console.log('üéµ Playing audio chunk, length:', audioData.length);
          
          // Decode base64 audio data from Gemini Live (24kHz PCM)
          const audioBuffer = await decodeAudioData(
            audioData,
            sessionManager.current.outputAudioContext,
            24000, // Gemini Live outputs 24kHz
            1      // Mono
          );
          
          if (!audioBuffer) {
            console.error('‚ùå Failed to decode audio buffer');
            resolve();
            return;
          }
          
          console.log('‚úÖ Audio chunk decoded:', {
            duration: audioBuffer.duration.toFixed(2) + 's',
            samples: audioBuffer.length
          });
          
          const source = sessionManager.current.outputAudioContext.createBufferSource();
          source.buffer = audioBuffer;
          source.connect(sessionManager.current.outputNode!);
          
          // Wait for this chunk to finish before playing next
          source.onended = () => {
            console.log('‚úÖ Audio chunk completed');
            resolve();
          };
          
          source.onerror = (error) => {
            console.error('‚ùå Audio source error:', error);
            resolve(); // Continue with next chunk even if this fails
          };
          
          source.start();
          console.log('üîä Started audio chunk playback');
          onStatusChange?.('üîä Playing AI response...');
          
        } catch (error) {
          console.error('‚ùå Audio chunk playback error:', error);
          resolve(); // Continue with next chunk
        }
      });
    };

    const handleMedicalFunctionCall = async (functionCall: any) => {
      try {
        console.log('üè• Medical Function Call:', functionCall.name, functionCall.args);
        
        // Build medical context
        const context = {
          patientId: patientContext?.id,
          patientContext,
          pageContext,
          userId: 'current_doctor', // Replace with actual auth
          sessionId: sessionManager.current.conversationId
        };

        // Find and execute function
        const func = medicalAtomicFunctions.find(f => f.name === functionCall.name);
        if (!func) {
          return {
            success: false,
            error: `Medical function ${functionCall.name} not found`,
            message: "I don't have access to that medical function yet."
          };
        }

        const result = await func.execute(functionCall.args, context);
        
        // Format response for voice
        const voiceMessage = formatMedicalResponseForVoice(result, functionCall.name);
        
        return {
          ...result,
          voiceMessage,
          type: 'medical_update',
          timestamp: new Date().toISOString()
        };
        
      } catch (error) {
        console.error('Medical function error:', error);
        return {
          success: false,
          error: error instanceof Error ? error.message : 'Unknown error',
          message: "I encountered an issue with that medical query. Please try again."
        };
      }
    };

    const startSpeechRecognition = () => {
      try {
        // Check if browser supports Speech Recognition
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
          console.log('üé§ Speech Recognition not supported in this browser');
          return;
        }

        speechRecognition.current = new SpeechRecognition();
        speechRecognition.current.continuous = true;
        speechRecognition.current.interimResults = true;
        speechRecognition.current.lang = language;
        
        console.log('üåê Speech recognition language set to:', language);

        speechRecognition.current.onstart = () => {
          console.log('üé§ Speech recognition started');
        };

        speechRecognition.current.onresult = (event) => {
          let interimTranscript = '';
          let finalTranscript = '';

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
              finalTranscript += transcript + ' ';
            } else {
              interimTranscript += transcript;
            }
          }

          // Update live speech display with interim results
          const currentSpeech = finalTranscript + interimTranscript;
          currentSpeechBuffer.current = currentSpeech;
          
          // Send live speech updates to UI
          onLiveSpeechUpdate?.(currentSpeech.trim());

          // When speech is final, add to transcript and send to Gemini Live
          if (finalTranscript) {
            const finalText = finalTranscript.trim();
            console.log('üé§ Final speech recognized:', finalText);
            
            onTranscriptUpdate?.({
              id: Date.now().toString(),
              type: 'user',
              content: finalText,
              timestamp: new Date().toISOString()
            });

            // EXPERIMENTAL: Try sending transcribed text as backup
            // Some models might need text input to trigger responses
            if (sessionManager.current.session && sessionReady.current) {
              try {
                console.log('üí¨ Sending transcribed text to ensure AI responds:', finalText);
                sessionManager.current.session.sendClientContent({
                  turns: [{
                    role: 'user',
                    parts: [{
                      text: finalText
                    }]
                  }]
                });
              } catch (error) {
                console.log('üí¨ Text sending error:', error);
              }
            }
          }
        };

        speechRecognition.current.onerror = (event) => {
          if (event.error === 'no-speech') {
            // This is normal - speech recognition times out when no speech detected
            // Only restart if we're still recording and not already starting
            setTimeout(() => {
              if (sessionManager.current.isRecording && speechRecognition.current) {
                try {
                  speechRecognition.current.start();
                } catch (error) {
                  console.log('üé§ Speech recognition already running, skipping restart');
                }
              }
            }, 1000);
          } else {
            // Only log actual errors, not normal timeouts
            console.error('üé§ Speech recognition error:', event.error);
          }
        };

        speechRecognition.current.onend = () => {
          console.log('üé§ Speech recognition ended');
          // Restart if still recording and not already running
          if (sessionManager.current.isRecording && speechRecognition.current) {
            setTimeout(() => {
              if (speechRecognition.current) {
                try {
                  speechRecognition.current.start();
                } catch (error) {
                  console.log('üé§ Speech recognition already running, skipping restart');
                }
              }
            }, 100);
          }
        };

        speechRecognition.current.start();
        console.log('üé§ Speech recognition initialized and started');

      } catch (error) {
        console.error('Failed to start speech recognition:', error);
      }
    };

    const stopSpeechRecognition = () => {
      if (speechRecognition.current) {
        speechRecognition.current.stop();
        speechRecognition.current = null;
        currentSpeechBuffer.current = '';
        onLiveSpeechUpdate?.('');
        console.log('üé§ Speech recognition stopped');
      }
    };

    const startAudioCapture = async () => {
      // Initialize audio contexts with proper sample rates
      if (!sessionManager.current.inputAudioContext) {
        // Don't force sample rate - let the browser choose optimal rate
        sessionManager.current.inputAudioContext = new AudioContext();
        sessionManager.current.inputNode = sessionManager.current.inputAudioContext.createGain();
      }

      if (!sessionManager.current.outputAudioContext) {
        sessionManager.current.outputAudioContext = new AudioContext({ sampleRate: 24000 });
        sessionManager.current.outputNode = sessionManager.current.outputAudioContext.createGain();
        sessionManager.current.outputNode.connect(sessionManager.current.outputAudioContext.destination);
      }

      // CRITICAL: Resume audio context BEFORE any other operations
      if (sessionManager.current.inputAudioContext.state === 'suspended') {
        console.log('‚ñ∂Ô∏è Resuming suspended audio context BEFORE setup...');
        await sessionManager.current.inputAudioContext.resume();
        console.log('‚úÖ Audio context resumed, state:', sessionManager.current.inputAudioContext.state);
      }

      // Get microphone stream
      console.log('üé§ Requesting microphone access...');
      try {
        sessionManager.current.mediaStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        console.log('‚úÖ Microphone access granted:', sessionManager.current.mediaStream);
        console.log('üìä Audio tracks:', sessionManager.current.mediaStream.getAudioTracks());
        
        // Verify stream is active
        const track = sessionManager.current.mediaStream.getAudioTracks()[0];
        console.log('üîç Audio track state:', {
          enabled: track.enabled,
          muted: track.muted,
          readyState: track.readyState,
          settings: track.getSettings()
        });
      } catch (error) {
        console.error('‚ùå Microphone access failed:', error);
        throw error;
      }

      const source = sessionManager.current.inputAudioContext.createMediaStreamSource(
        sessionManager.current.mediaStream
      );
      console.log('üéµ Audio source created:', source);

      try {
        // Use modern AudioWorklet instead of deprecated ScriptProcessorNode
        console.log('üîß Loading AudioWorklet for modern audio processing...');
        await sessionManager.current.inputAudioContext.audioWorklet.addModule('/audio-worklet-processor.js');
        console.log('‚úÖ AudioWorklet module loaded');
        
        // Create AudioWorkletNode
        sessionManager.current.audioWorkletNode = new AudioWorkletNode(
          sessionManager.current.inputAudioContext,
          'medical-audio-processor'
        );
        
        console.log('üîß AudioWorklet processor created:', sessionManager.current.audioWorkletNode);
        
        // Set up message handling with voice activity detection
        let audioProcessCount = 0;
        let speechBuffer: Float32Array[] = [];
        let silenceCount = 0;
        let lastSpeechTime = Date.now();
        
        sessionManager.current.audioWorkletNode.port.onmessage = (event) => {
          audioProcessCount++;
          
          const { type, data, message } = event.data;
          
          switch (type) {
            case 'log':
              console.log(message);
              break;
              
            case 'audioData':
              if (!sessionManager.current.isRecording || !sessionManager.current.session || !sessionReady.current) {
                return;
              }
              
              const { audioData, volume } = data;
              const float32Data = new Float32Array(audioData);
              
              // Voice Activity Detection
              if (volume > 0.01) { // Speech detected
                speechBuffer.push(float32Data);
                silenceCount = 0;
                lastSpeechTime = Date.now();
                onStatusChange?.('üéôÔ∏è Listening...');
                
                // Send real-time audio data to Gemini Live (native audio model needs this)
                try {
                  const audioBlob = createAudioBlob(float32Data);
                  
                  // Log audio data details for debugging (reduce frequency)
                  if (audioProcessCount % 100 === 0 || audioProcessCount <= 5) {
                    console.log('üé§ Capturing speech #' + audioProcessCount + ':', {
                      volume: volume.toFixed(4),
                      bufferLength: speechBuffer.length,
                      timestamp: Date.now()
                    });
                  }
                  
                  // Send audio to native audio model (this is what it expects)
                  if (sessionManager.current.session?.sendRealtimeInput && sessionManager.current.isRecording) {
                    sessionManager.current.session.sendRealtimeInput({
                      media: {
                        mimeType: audioBlob.mimeType,
                        data: audioBlob.data
                      }
                    });
                    
                    if (audioProcessCount % 100 === 0) {
                      console.log('‚úÖ Audio chunk #' + audioProcessCount + ' sent to Gemini Live');
                    }
                  }
                } catch (error) {
                  console.error('‚ùå Audio send error:', error);
                  // Don't flood console if session is closing
                  if (!error.message?.includes('CLOSING') && !error.message?.includes('CLOSED')) {
                    console.error('‚ùå Unexpected audio error:', error);
                  }
                }
              } else {
                // Silence detected
                silenceCount++;
                
                // If we had speech and now have silence for 1 second, signal end of speech
                if (speechBuffer.length > 0 && silenceCount > 40) { // ~1 second at 44.1kHz
                  console.log('üõë End of speech detected via audio worklet');
                  
                  // Signal to Gemini Live that user has finished speaking
                  try {
                    if (sessionManager.current.session?.sendRealtimeInput) {
                      console.log('üì§ Sending end-of-speech signal to Gemini Live');
                      // Send empty input to signal completion
                      sessionManager.current.session.sendRealtimeInput({
                        media: {
                          mimeType: 'audio/pcm;rate=16000',
                          data: '' // Empty data signals end of turn
                        }
                      });
                    }
                  } catch (error) {
                    console.log('üì§ End-of-speech signal error (may be normal):', error);
                  }
                  
                  // Reset speech buffer
                  speechBuffer = [];
                  silenceCount = 0;
                  onStatusChange?.('ü§î Processing your request...');
                }
              }
              break;
          }
        };
        
        // Connect audio pipeline: microphone -> worklet -> destination
        console.log('üîó Connecting modern AudioWorklet pipeline...');
        source.connect(sessionManager.current.audioWorkletNode);
        sessionManager.current.audioWorkletNode.connect(sessionManager.current.inputAudioContext.destination);
        
        console.log('üîó AudioWorklet pipeline connected:', {
          source: source,
          workletNode: sessionManager.current.audioWorkletNode,
          destination: sessionManager.current.inputAudioContext.destination,
          contextState: sessionManager.current.inputAudioContext.state,
          sampleRate: sessionManager.current.inputAudioContext.sampleRate
        });
        
        // Start the worklet processor
        sessionManager.current.audioWorkletNode.port.postMessage({ type: 'start' });
        
        // Audio processing is working - no need for test
        
      } catch (workletError) {
        console.error('‚ùå AudioWorklet failed, falling back to ScriptProcessorNode:', workletError);
        
        // Fallback to ScriptProcessorNode for older browsers
        console.log('üîß Setting up ScriptProcessorNode fallback...');
        const bufferSize = 2048;
        sessionManager.current.scriptProcessorNode = sessionManager.current.inputAudioContext.createScriptProcessor(
          bufferSize, 1, 1
        );

        let audioProcessCount = 0;
        let lastLogTime = Date.now();
        
        sessionManager.current.scriptProcessorNode.onaudioprocess = (event) => {
          audioProcessCount++;
          const now = Date.now();
          
          if (audioProcessCount <= 20 || (now - lastLogTime) > 2000) {
            console.log(`üé§ ScriptProcessor fallback #${audioProcessCount}, timestamp: ${now}`);
            lastLogTime = now;
          }
          
          if (!sessionManager.current.isRecording) {
            if (audioProcessCount <= 5) console.log('‚è∏Ô∏è Not recording, skipping audio');
            return;
          }
          
          if (!sessionManager.current.session) {
            if (audioProcessCount <= 5) console.log('‚ùå No session available, skipping audio');
            return;
          }
          
          const inputBuffer = event.inputBuffer;
          const pcmData = inputBuffer.getChannelData(0);
          
          const volume = Math.sqrt(pcmData.reduce((sum, sample) => sum + sample * sample, 0) / pcmData.length);
          
          if (volume > 0.0001 || audioProcessCount <= 15) {
            console.log(`üé§ ScriptProcessor #${audioProcessCount}, volume: ${volume.toFixed(6)}, buffer: ${pcmData.length} samples`);
          }
          
          if (volume > 0.003) {
            onStatusChange?.('üéôÔ∏è Listening to medical query...');
            
            // Note: Audio sending disabled in fallback mode too
            // Speech recognition handles all voice-to-text conversion
            if (audioProcessCount % 100 === 0) {
              console.log('üé§ ScriptProcessor fallback volume:', volume.toFixed(4));
            }
          }
        };

        // Connect fallback pipeline
        source.connect(sessionManager.current.scriptProcessorNode);
        sessionManager.current.scriptProcessorNode.connect(sessionManager.current.inputAudioContext.destination);
        
        console.log('üîó ScriptProcessorNode fallback pipeline connected');
      }
    };


    const setMuted = (muted: boolean) => {
      if (sessionManager.current.outputNode) {
        sessionManager.current.outputNode.gain.value = muted ? 0 : 1;
      }
    };

    const setVolume = (volume: number) => {
      if (sessionManager.current.outputNode) {
        sessionManager.current.outputNode.gain.value = Math.max(0, Math.min(1, volume));
      }
    };

    const sendTestMessage = (message: string) => {
      if (sessionManager.current.session && sessionReady.current) {
        try {
          console.log('üß™ Sending test message:', message);
          sessionManager.current.session.sendClientContent({
            turns: [{
              role: 'user',
              parts: [{
                text: message
              }]
            }]
          });
        } catch (error) {
          console.error('Test message error:', error);
          onError?.(`Test message failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
      } else {
        console.log('‚ùå Session not ready for test message');
        onError?.('Session not ready - please start recording first');
      }
    };

    useEffect(() => {
      if (!initRef.current) {
        initRef.current = true;
        
        // Initialize session manager with status callback
        sessionManager.current.onStatusChange = onStatusChange;
        
        // Restore any existing session state
        if (sessionManager.current.currentStatus) {
          onStatusChange?.(sessionManager.current.currentStatus);
        }
      }

      return () => {
        // Component cleanup - don't kill session, just cleanup listeners
        if (sessionManager.current.scriptProcessorNode) {
          sessionManager.current.scriptProcessorNode.onaudioprocess = null;
        }
      };
    }, [onStatusChange]);

    // This component is headless - no rendering
    return null;
  }
);

MedicalAudioService.displayName = 'MedicalAudioService';